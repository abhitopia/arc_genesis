model:
  model_type: slot_attention
  K: 5
  in_chnls: 3
  img_size: 64
  feat_dim: 256
  broadcast_size: 64
  add_coords_every_layer: false
  normal_std: 0.7
  use_vae: true
  lstm_hidden_dim: 256
  num_layers: 4
  num_iterations: 5
  num_heads: 4
  slot_dim: 256
  implicit_grads: true
training:
  batch_size: 128
  max_steps: 500000
  learning_rate: 0.0004
  weight_decay: 0.0
  use_lr_scheduler: true
  warmup_steps: 10000
  warmup_start_lr: 1.0e-06
  cosine_decay_steps: null
  cosine_min_lr: 5.0e-06
  mixture_weight: 0.0
  mse_weight: 1.0
  latent_kl_weight: 0.0
  mask_kl_weight: 0.0
  elbo_divergence_threshold: 100000000.0
  use_geco: false
  geco_goal: 0.5655
  geco_lr: 1.0e-05
  geco_alpha: 0.99
  geco_beta_init: 1.0
  geco_beta_min: 1.0e-10
  geco_speedup: 10.0
data:
  dataset: d_sprites
  variable_size: false
  is_discrete: false
  seed: 42
project_name: genesis_v2
run_name: slot_attention_lr_k4
save_dir: ./experiments
log_every_n_steps: 10
val_check_interval: 500
num_visualizations: 8
save_top_k: 3
monitor_metric: loss_val/total_loss
monitor_mode: min
